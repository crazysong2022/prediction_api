import os
import requests
import json
import time
import streamlit as st
import pandas as pd
from dotenv import load_dotenv
from utils.db_utils import get_db_connection
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Polymarketæ•°æ®åŒæ­¥å·¥å…·",
    page_icon="ğŸ“Š",
    layout="wide"
)

# åº”ç”¨æ ‡é¢˜
st.title("Polymarketæ•°æ®åŒæ­¥å·¥å…·")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("é…ç½®")
    
    # APIé…ç½®
    st.subheader("APIè®¾ç½®")
    API_URL = st.text_input("API URL", "https://gamma-api.polymarket.com/markets")
    start_date = st.date_input("å¼€å§‹æ—¥æœŸ", value=pd.to_datetime("2025-01-01"))
    end_date = st.date_input("ç»“æŸæ—¥æœŸ", value=pd.to_datetime("2025-01-02"))
    limit = st.number_input("è¿”å›æ•°é‡", min_value=1, max_value=100, value=2)
    
    # ä»£ç†é…ç½®
    use_proxy = st.checkbox("ä½¿ç”¨ä»£ç†æœåŠ¡å™¨")
    if use_proxy:
        http_proxy = st.text_input("HTTPä»£ç†", "")
        https_proxy = st.text_input("HTTPSä»£ç†", "")
        proxy_dict = {
            'http': http_proxy,
            'https': https_proxy
        }
    else:
        proxy_dict = None
    
    # è‡ªåŠ¨åˆ·æ–°é…ç½®
    st.subheader("è‡ªåŠ¨åˆ·æ–°")
    auto_refresh = st.checkbox("å¯ç”¨è‡ªåŠ¨åˆ·æ–°")
    refresh_interval = st.slider("åˆ·æ–°é—´éš”(åˆ†é’Ÿ)", min_value=1, max_value=60, value=30)

# ç¼“å­˜APIè¯·æ±‚å‡½æ•°ï¼Œé¿å…é‡å¤è¯·æ±‚
@st.cache_data(ttl=300)
def fetch_polymarket_data(url: str, start_date: str, end_date: str, limit: int, proxy: dict = None) -> dict:
    """ä»Polymarket APIè·å–å¸‚åœºæ•°æ®ï¼Œæ·»åŠ è¶…æ—¶å’Œé‡è¯•æœºåˆ¶"""
    querystring = {
        "start_date_min": f"{start_date}T00:00:00Z",
        "end_date_min": f"{end_date}T00:00:00Z",
        "limit": str(limit)
    }
    
    # é…ç½®é‡è¯•ç­–ç•¥
    session = requests.Session()
    retries = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[500, 502, 503, 504]
    )
    session.mount('https://', HTTPAdapter(max_retries=retries))
    
    try:
        # è®¾ç½®è¶…æ—¶
        response = session.get(url, params=querystring, proxies=proxy, timeout=(5, 10))
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"APIè¯·æ±‚é”™è¯¯: {e}")
        return None

def save_to_database(content: dict) -> None:
    """å°†å†…å®¹ä¿å­˜åˆ°æ•°æ®åº“çš„testè¡¨ä¸­ï¼Œè‹¥å­˜åœ¨åˆ™è¦†ç›–"""
    if not content:
        st.warning("æ²¡æœ‰å†…å®¹å¯ä¿å­˜")
        return
    
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                # å…ˆæ¸…ç©ºè¡¨
                cursor.execute("DELETE FROM test")
                
                # æ’å…¥æ–°æ•°æ®
                cursor.execute(
                    "INSERT INTO test (info) VALUES (%s)",
                    (json.dumps(content),)
                )
            
            conn.commit()
            st.success("æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")
    except Exception as e:
        st.error(f"æ•°æ®åº“æ“ä½œé”™è¯¯: {e}")

# ä¸»ç•Œé¢
col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("APIæ•°æ®")
    
    # æ‰‹åŠ¨è§¦å‘æŒ‰é’®
    if st.button("ç«‹å³åŒæ­¥æ•°æ®"):
        with st.spinner("æ­£åœ¨è·å–æ•°æ®..."):
            api_data = fetch_polymarket_data(
                API_URL, 
                start_date.strftime("%Y-%m-%d"), 
                end_date.strftime("%Y-%m-%d"), 
                limit,
                proxy_dict
            )
            
            if api_data:
                st.json(api_data)
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                with st.spinner("æ­£åœ¨ä¿å­˜åˆ°æ•°æ®åº“..."):
                    save_to_database(api_data)
            else:
                st.warning("æœªèƒ½è·å–åˆ°æ•°æ®")

with col2:
    st.subheader("æ•°æ®åº“çŠ¶æ€")
    
    # æ˜¾ç¤ºæœ€åæ›´æ–°æ—¶é—´
    try:
        with get_db_connection() as conn:
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM test")
                count = cursor.fetchone()[0]
                
                if count > 0:
                    st.info(f"æ•°æ®åº“ä¸­æœ‰ {count} æ¡è®°å½•")
                    
                    # æ˜¾ç¤ºæœ€åæ›´æ–°æ—¶é—´
                    cursor.execute("SELECT pg_xact_commit_timestamp(xmin) FROM test LIMIT 1")
                    last_updated = cursor.fetchone()[0]
                    st.write(f"æœ€åæ›´æ–°æ—¶é—´: {last_updated}")
                    
                    # é¢„è§ˆæ•°æ®
                    if st.button("æŸ¥çœ‹æ•°æ®åº“å†…å®¹"):
                        cursor.execute("SELECT info FROM test LIMIT 1")
                        data = cursor.fetchone()[0]
                        st.json(json.loads(data))
                else:
                    st.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®ï¼Œè¯·å…ˆåŒæ­¥")
    except Exception as e:
        st.error(f"æ— æ³•è¿æ¥åˆ°æ•°æ®åº“: {e}")

# è‡ªåŠ¨åˆ·æ–°é€»è¾‘
if auto_refresh:
    st.sidebar.info(f"è‡ªåŠ¨åˆ·æ–°å·²å¯ç”¨ï¼Œæ¯ {refresh_interval} åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡")
    
    if 'last_refresh_time' not in st.session_state:
        st.session_state.last_refresh_time = time.time()
    
    current_time = time.time()
    if current_time - st.session_state.last_refresh_time > refresh_interval * 60:
        st.session_state.last_refresh_time = current_time
        
        with st.spinner("è‡ªåŠ¨åˆ·æ–°ä¸­..."):
            api_data = fetch_polymarket_data(
                API_URL, 
                start_date.strftime("%Y-%m-%d"), 
                end_date.strftime("%Y-%m-%d"), 
                limit,
                proxy_dict
            )
            
            if api_data:
                save_to_database(api_data)
                st.experimental_rerun()  # åˆ·æ–°é¡µé¢    